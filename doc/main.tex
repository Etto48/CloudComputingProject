\documentclass{article}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{xcolor}
\definecolor{keyword}{rgb}{0.3568627451, 0.7372549020, 0.6431372549}
\definecolor{comment}{rgb}{0.5411764706, 0.7490196078, 0.3450980392}
\definecolor{string}{rgb}{0.9,0.4,0}
\definecolor{function}{rgb}{0.5,0.5,0.1}
\usepackage{listings}
\lstset{
    basicstyle=\small\ttfamily,
    breaklines=true,
    frame=single,
    rulecolor = \color{gray},
    numbers=left,
    numbersep=5pt,
    numberstyle=\tiny,
    showstringspaces=false,
    tabsize=4,
    keywordstyle=\color{keyword},
    commentstyle=\color{comment},
    stringstyle=\color{string},
}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    urlcolor=cyan,
}
\usepackage{makecell}
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{float}

\title{Cloud Computing - Project Report \\ \small{Totoro Group}}
\author{
    Zahra Omrani \\ z.omrani@studenti.unipi.it \and 
    Paolo Palumbo \\ p.palumbo3@studenti.unipi.it \and
    Ettore Ricci \\ e.ricci32@studenti.unipi.it}

\begin{document}
\maketitle
\begin{center}
    \scriptsize
    \href{https://github.com/Etto48/CloudComputingProject}{https://github.com/Etto48/CloudComputingProject}
\end{center}

\begin{abstract}
    This report presents the development and performance evaluation of a Java-based application 
    designed to count letter frequencies in large datasets using the Hadoop framework. 
    To provide a comprehensive analysis, we also implemented and tested non-distributed 
    applications in Python and Rust for comparison.
    The results show that for the sizes of the datasets considered, Java and Python execution 
    times are comparable, while Rust is significantly faster. 
    Both Python and Rust applications used significantly less memory than the Java application.
\end{abstract}
\begin{multicols}{2}
\section{Introduction}
    Our project for the Cloud Computing course consists of developing a Java application to count
    the frequency of letters in a large dataset using the Hadoop framework.
    The main objective of the project is to analyze and compare the performance of the application
    with different configurations and input sizes and also to compare it with non-distributed
    implementations in Python and Rust.
\section{Mapreduce}
    MapReduce is a programming model for processing large data sets. 
    Users specify a map function that processes a key/value pair to generate a set
    of intermediate key/value pairs, and a reduce function that merges all intermediate values
    associated with the same intermediate key.
    For our application we needed two subsequent MapReduce jobs: the first one to count the occurrences
    of each letter in the input text and the second one to calculate the frequency of each letter.
    \subsection{Job 1: Counting letters}
        \subsubsection{Mapper}
            For testing purposes, we created two different mappers: one with an in-mapper combiner and one 
            without.
            For the combiner (when enabled), we used the same code as the reducer.
            \begin{itemize}
                \item \textbf{Mapper with in-mapper combiner}:
                This mapper stores the letter counts in a vector and emits them in the cleanup method.
                At the end of the cleanup method, the mapper increments a job counter with the number of emitted 
                letters.
                (Listing \ref{lst:java_mapper})
                \item \textbf{Mapper without in-mapper combiner}: 
                This mapper emits the letter counts (with value 1) for each letter in the map method.
                At the end of the map method, the mapper increments a job counter with the number of emitted
                letters.
                (Listing \ref{lst:java_mapper_no_combiner})
            \end{itemize}
        \subsubsection{Reducer}
            The reducer receives the letter counts from the mappers and sums them up.
            (Listing \ref{lst:java_reducer})
    \subsection{Job 2: Calculating frequencies}
        The results of the first job are saved in a temporary file and used as input for the second job.
        The counter obtained from the first job is stored in a configuration variable that is passed to the 
        reducer (only one is needed because the number of different letters is small).
        For this job we do not need a combiner so it's always disabled.
        \subsubsection{Mapper}
            We used an identity mapper that emits the letter counts as they are.
        \subsubsection{Reducer}
            The reducer receives the letter counts from the mappers and calculates the frequency of each letter
            using the total count that was previously stored in a configuration variable.
            (Listing \ref{lst:java_reducer_frequency})
\section{Dataset}
    We used three different datasets:
    \begin{itemize}
        \item \textbf{english.txt}: A 1.2GB text file containing text from the books on \href{https://www.gutenberg.org/}{Gutenberg project}.
        \item \textbf{italian.txt}: A 1.3GB text file from the \href{https://www.corpusitaliano.it/}{PAISÃ€ corpus}\cite{lyding-etal-2014-paisa}.
        \item \textbf{spanish.txt}: A 130MB text file from the \href{https://wortschatz.uni-leipzig.de/en/download/Spanish}{Leipzig Corpora Collection}\cite{Eckart2013}.
    \end{itemize}
    We also created 11 files of increasing size (from 100MB to 1.1GB with 100MB steps) to test the performance
    of the application with different input sizes. These files are called \textbf{part\_$X$MB.txt} and 
    contain the first $X$MBs of the \textbf{english.txt} file. 
\section{Experiments}
    We wrote our program in a way that allows us to configure it for the different tests with 
    command line arguments:
    \begin{itemize}
        \item \textbf{$-i|--input$}: Input file path inside the HDFS.
        \item \textbf{$-r|--reducers$}: Number of reducers to use in the first job.
        \item \textbf{$-n|--no-combiner$}: Disable the combiner in the first job.
        \item \textbf{$-m|--no-in-mapper-combiner$}: Disable the in-mapper combiner in the first job.
        \item \textbf{$-o|--output$}: Output directory path inside the HDFS, 
        by default it's set to \textbf{output}.
        \item \textbf{$-t|--tmp$}: Temporary directory path inside the HDFS,
        by default it's set to \textbf{tmp}.
    \end{itemize}
    We carried out 21 tests with the configurations shown in 
    Table \ref{tab:general_tests} and Table \ref{tab:input_split_tests}.
    Originally we ran tests 7, 8 and 9 with 4 reducers, but test 9 could never complete because of a 
    Shuffle Error caused by a Java Heap Space error (Out Of Memory Error). Because of this, 
    we decided to run these tests again with 3 reducers.
    Only the 3 reducers configuration will be shown in the results.
    We also ran the Python and Rust applications with the same input file (english.txt).
    \begin{table}[H]
        \centering
        \begin{tabular}{|c|l|}
            \hline
            Test ID & Arguments \\
            \hline
            0 & \makecell[l]{-i english.txt \\ -r 1} \\        
            \hline
            1 & \makecell[l]{-i english.txt \\ -r 2} \\        
            \hline
            2 & \makecell[l]{-i english.txt \\ -r 4} \\        
            \hline
            3 & \makecell[l]{-i english.txt \\ -r 8} \\        
            \hline
            4 & \makecell[l]{-i english.txt \\ -r 1 \\ --no-combiner} \\        
            \hline
            5 & \makecell[l]{-i english.txt \\ -r 1 \\ --no-in-mapper-combiner} \\        
            \hline
            6 & \makecell[l]{-i english.txt \\ -r 1 \\ --no-in-mapper-combiner \\ --no-combiner} \\        
            \hline
            7 & \makecell[l]{-i english.txt \\ -r 3 \\ --no-combiner} \\        
            \hline
            8 & \makecell[l]{-i english.txt \\ -r 3 \\ --no-in-mapper-combiner} \\        
            \hline
            9 & \makecell[l]{-i english.txt \\ -r 3 \\ --no-in-mapper-combiner \\ --no-combiner} \\        
            \hline
        \end{tabular}
        \caption{General tests}
        \label{tab:general_tests}
    \end{table}
    \begin{table}[H]
        \centering
        \begin{tabular}{|c|l|}
            \hline
            Test ID & Arguments \\
            \hline
            10 & \makecell[l]{-i part\_100MB.txt \\ -r 1} \\  
            \hline      
            11 & \makecell[l]{-i part\_200MB.txt \\ -r 1} \\  
            \hline      
            12 & \makecell[l]{-i part\_300MB.txt \\ -r 1} \\        
            \hline
            13 & \makecell[l]{-i part\_400MB.txt \\ -r 1} \\        
            \hline
            14 & \makecell[l]{-i part\_500MB.txt \\ -r 1} \\        
            \hline
            15 & \makecell[l]{-i part\_600MB.txt \\ -r 1} \\        
            \hline
            16 & \makecell[l]{-i part\_700MB.txt \\ -r 1} \\        
            \hline
            17 & \makecell[l]{-i part\_800MB.txt \\ -r 1} \\        
            \hline
            18 & \makecell[l]{-i part\_900MB.txt \\ -r 1} \\        
            \hline
            19 & \makecell[l]{-i part\_1000MB.txt \\ -r 1} \\        
            \hline
            20 & \makecell[l]{-i part\_1100MB.txt \\ -r 1} \\            
            \hline
        \end{tabular}
        \caption{Input split tests}
        \label{tab:input_split_tests}
    \end{table}
\section{Results}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/en.png}
    \caption{English letter frequency}
    \label{fig:en_freq}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/it.png}
    \caption{Italian letter frequency}
    \label{fig:it_freq}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/es.png}
    \caption{Spanish letter frequency}
    \label{fig:es_freq}
\end{figure}



\end{multicols}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{figures/experiments.png}
    \caption{Tests results, the x-axis represents the test ID.}
    \label{fig:tests_graph}
\end{figure}
\begin{multicols}{2}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/baseline_py.png}
    \caption{Python baseline memory usage (GB) over time (s)}
    \label{fig:baseline_py}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/baseline_rs.png}
    \caption{Rust baseline memory usage (GB) over time (s)}
    \label{fig:baseline_rs}
\end{figure}

\section{Conclusions}
\end{multicols}
\section{Listings}
\lstinputlisting[
    language=Java, 
    caption={Mapper with in-mapper combiner},
    label={lst:java_mapper}
]{../letterfreq/src/main/java/it/unipi/Mapper.java}
\lstinputlisting[
    language=Java,
    caption={Mapper without in-mapper combiner},
    label={lst:java_mapper_no_combiner}
]{../letterfreq/src/main/java/it/unipi/MapperNoCombiner.java}
\lstinputlisting[
    language=Java,
    caption={Reducer},
    label={lst:java_reducer}
]{../letterfreq/src/main/java/it/unipi/Reducer.java}
\lstinputlisting[
    language=Java,
    caption={ReducerFrequency class, used to calculate the frequency of each letter},
    label={lst:java_reducer_frequency}
]{../letterfreq/src/main/java/it/unipi/ReducerFrequency.java}
\lstinputlisting[
    language=Java,
    caption={Char class, used as a custom key},
    label={lst:java_char}
]{../letterfreq/src/main/java/it/unipi/Char.java}

\bibliographystyle{IEEETran}
\bibliography{references.bib}
\end{document}